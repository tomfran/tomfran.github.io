<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Log-Structured Merge Tree | </title>
<meta name="keywords" content="database, java">
<meta name="description" content="An LSM Tree overview and Java implementation">
<meta name="author" content="">
<link rel="canonical" href="https://tomfran.github.io/posts/lsm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6fe192c77f84ab2f13953fbfe17ad4b7982d1656d8ea2c1ec9b81c1c4671710a.css" integrity="sha256-b&#43;GSx3&#43;Eqy8TlT&#43;/4XrUt5gtFlbY6iweybgcHEZxcQo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tomfran.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tomfran.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tomfran.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tomfran.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tomfran.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-1EM39PGLW2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-1EM39PGLW2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Log-Structured Merge Tree" />
<meta property="og:description" content="An LSM Tree overview and Java implementation" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tomfran.github.io/posts/lsm/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-11-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Log-Structured Merge Tree"/>
<meta name="twitter:description" content="An LSM Tree overview and Java implementation"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://tomfran.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Log-Structured Merge Tree",
      "item": "https://tomfran.github.io/posts/lsm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Log-Structured Merge Tree",
  "name": "Log-Structured Merge Tree",
  "description": "An LSM Tree overview and Java implementation",
  "keywords": [
    "database", "java"
  ],
  "articleBody": "I studied LSM trees at university and after encountering them twice in Designing Data-Intensive Applications and Database Internals I decided to implement something in Java.\nThe idea behind this project is not to provide the most efficient implementation ever, but to experiment with storing data on disk, any suggestions are welcome!\nHere’s the Github repo if you want to have a look, this article is also published on Medium.\nIntroduction An LSM tree is a structure used by NoSQL databases, such as Cassandra, RocksDB, LevelDB, Dynamo, and so on. It’s suitable for write-intensive applications.\nWe can distinguish two key components of the tree, the in-memory buffer, also called Memtable, and the disk-resident tables. The main idea is to accept writes to the in-memory part of the tree, and to flush them periodically, or when a certain size is met.\nA key aspect of this structure is ordering, indeed, keys are sorted both in RAM and on disk, enabling logarithmic searches.\nFor the sake of this project elements in the tree are simple key-value pairs.\nMemtable Having sorted elements in memory is not a new problem we can exploit any efficient order-preserving data structure for this part, such as Red-Black or AVL trees.\nIn this particular implementation, I decided to build a Skip List, which provides the same theoretical complexity in the average case of balanced trees, but is straightforward to implement. 1\nA Skip List is a multi-leveled linked list. The idea is to have fast lanes between nodes, and, by carefully constructing them, we can reduce the number of links we need to traverse while searching.\nThe list properties are:\nelements at level zero are sorted; the number of levels are $\\log(n)$, where $n$ is the size of the list; if a node is at level $i$, then is must also be at level $i-1$. Searching Given the above properties, searching is done as follows:\nstart at the highest level and traverse until the node key is less than the wanted key; if the successor surpasses the wanted key, go down a level and repeat, else we found the element. Eventually, we’ll reach level zero, and determine if the element is found or not. Inserting Insertion proceeds as follows:\nlocate the insert position with the same logic as before; determine a level for the new element; insert as in a linked list, but at each required level. Note that for this to work we need to keep track of a predecessor buffer while descending levels. This way we can correctly replace successors pointers at each level. public void add(ByteArrayPair item) { // Locate the element keeping track of predecessors at each level Node current = sentinel; for (int i = levels - 1; i \u003e= 0; i--) { while (current.next[i] != null \u0026\u0026 current.next[i].val.compareTo(item) \u003c 0) current = current.next[i]; buffer[i] = current; } // Replace current value if possible if (current.next[0] != null \u0026\u0026 current.next[0].val.compareTo(item) == 0) { current.next[0].val = item; return; } // Insert new node at a random level, updating predecessors Node newNode = new Node(item, levels); for (int i = 0; i \u003c randomLevel(); i++) { newNode.next[i] = buffer[i].next[i]; buffer[i].next[i] = newNode; } } Choosing a level To determine a level, we can toss a coin and keep going until we get heads. This would require a lot of random generations, a faster way is to generate a single number and use its binary representation as boolean values.\nprivate int randomLevel() { int level = 1; long n = rn.nextLong(); while (level \u003c levels \u0026\u0026 (n \u0026 (1L \u003c\u003c level)) != 0) level++; return level; } SSTable A Sorted String Table is a disk-based structure for sorted immutable data. They consist of two main files, one with actual data and another with an index to speed up look-ups.\nIndexing and Look-Ups Given the data file, searching for a key can be implemented with a full scan. This is tremendously slow on big files, hence we rely on indexing to skip portions of data.\nGiven a sampling factor $k$, we build a sparse index with keys at position $0, k, 2k$, and so on. By storing the index in an array we can rely on binary search to find a given offset in the data file, where we can start a linear scan. This permits us to skip a lot of unnecessary comparisons and locate a file portion that likely stores our value.\nNote that we can stop the search as soon as the current element surpasses the wanted one. Below is the code for searching, this implementation is as lazy as possible, meaning that we only read what’s strictly necessary while iterating on the input stream.\npublic byte[] get(byte[] key) { // binary search an offset to start search long offset = getCandidateOffsetIndex(key); int remaining = size - sparseSizeCount.getInt(offsetIndex); // move input stream to the offset given by the index is.seek(offset); int cmp = 1; int searchKeyLen = key.length, readKeyLen, readValueLen; byte[] readKey; while (cmp \u003e 0 \u0026\u0026 remaining \u003e 0) { remaining--; readKeyLen = is.readVByteInt(); // gone too far if (readKeyLen \u003e searchKeyLen) { return null; } // gone too short if (readKeyLen \u003c searchKeyLen) { readValueLen = is.readVByteInt(); is.skip(readKeyLen + readValueLen); continue; } // read full key, compare, if equal read value readValueLen = is.readVByteInt(); readKey = is.readNBytes(readKeyLen); cmp = compare(key, readKey); if (cmp == 0) { return is.readNBytes(readValueLen); } else { is.skip(readValueLen); } } return null; } Bloom Filters What happens when we search for a key that’s not on disk? We waste a lot of precious CPU cycles on binary searching and seeking on an offset, and iterating until we surpass the wanted key.\nTo avoid unnecessary operations we can rely on a compact and probabilistic structure such as Bloom Filters. The idea is to have a structure that answers membership queries, having some false positive answers, but no false negatives. We can tune the structure for our particular needs, by specifying a false-positive rate.\nSo, while looking for a key, we first test for probabilistic membership, and if the answer is negative, we can early return null from the search.\npublic byte[] get(byte[] key) { if (!bloomFilter.mightContain(key)) return null; ... } Data layout Data is disk-resident, hence we need to define a binary format to follow, with minimal overhead. An SSTable is made of $n$ elements, where each one of them has a variable length key and value.\nKey and value pairs are byte arrays, and to lay them out on disk we encode their length $l$, followed by $l$ bytes. Each integer is written in variable byte encoding, to not waste 32 bits on small numbers. 2\nThis encoding uses a byte to store a continuation bit and a 7-bit payload containing part of the represented number. For instance, consider the number $456$ and its binary representation $111001000$, the variable byte encoded version is:\n$$|1|0000011|0|1001000|$$\nThe first byte begins with one, hence that the number is not finished, while the second block starts with zero, indicating no more bytes are needed to decode the current integer.\nThe index file is a list of keys, so we can use the same length plus payload encoding for it. Each index entry has an offset related to it, specifying the number of bytes to skip in the file to reach it. Those offsets are increasing, so we can use something like delta-encoding to store them. Below is an example of such encoding: $$0, 25, 76, \\dots$$ $$\\downarrow$$ $$0, (25 - 0), (76 - 25), \\dots$$ Resulting integers are smaller, thus encoded in fewer bytes.\nFinally, Bloom Filters are represented by some hyperparameters and a bit vector, that we can encode as is.\nPutting it all together We saw how to construct an SStable and how a Skip List works in memory, It is time to combine the two to obtain the final engine.\nThe main components of the tree are:\nin-memory mutable buffer or Memtable: a skip list as presented in the previous paragraphs, with a max size; in-memory immutable buffers: a list of skip lists containing memtables that need to be flushed to disk; disk-resident tables: a collection of SSTables obtained from memtables flushing, they are divided into levels, level zero containing the most recent data. We are going to first see how primitives are defined, and then give an overview of how the tree is maintained, with buffer flushing and table compaction.\nInsertion and Search To insert a new element we simply add it to the in-memory buffer. If the list does not exceed the maximum size we are done, otherwise the current list is scheduled for disk flushing, and the mutable buffer is re-initialized.\nSearching is a bit trickier, and has at most three steps:\nquery the mutable buffer; query all the immutable buffers scheduled for flushing; query all the disk tables starting from level zero and on. If, at any point, the wanted key is found, we can stop the search.\npublic byte[] get(byte[] key) { byte[] result; if ((result = mutableMemtable.get(key)) != null) return result; for (Memtable memtable : immutableMemtables) if ((result = memtable.get(key)) != null) return result; for (LinkedList level : tables) for (SSTable table : level) if ((result = table.get(key)) != null) return result; return null; } Flushing the Memtable to disk When a given threshold is met, the Memtable is scheduled for flushing. To avoid blocking the whole Tree until data is persisted on disk, we use a background thread.\nprivate void checkMemtableSize() { if (mutableMemtable.size() \u003c= mutableMemtableMaxSize) return; synchronized (immutableMemtablesLock) { immutableMemtables.addFirst(mutableMemtable); mutableMemtable = new Memtable(mutableMemtableMaxSize); memtableFlusher.execute(this::flushLastMemtable); } } The background executor collects the older memtable to flush and creates a level-zero SSTable on disk. It is important to guard critical sections while doing such operations.\nTables compaction Flushing many Memtables on disk creates excessive read amplification, as we need to potentially query a lot of different structures to find the wanted element. One solution is to employ periodic compaction of disk tables.\nThe main idea is, starting from a collection of tables, to obtain a single merged one. There might exist conflicts, that need to be resolved with a last-write-wins principle.\nFor instance, given the following three tables, ordered by flushing time:\n$t_1 = [ a : 1, b : 2 ]$ $t_2 = [ b : 7, c : 3 ]$ $t_3 = [ a : 9, d : 9 ]$ The result after merging will be: $$t_4 = [a:1, b:2, c:3, d:9]$$\nFor the sake of this implementation, the compaction policy works by defining a maximum level size; once a level $l$ reaches the threshold, all its tables are compacted into a $l+1$ level table.\nImplementation-wise, this is equivalent to the problem of merging $k$ sorted iterators. The problem can be solved by using a priority queue to find the next element in $log(k)$ time complexity. 3\nConclusions Overall this was a really fun project, there were far more implementation challenges than I expected and some cool DSA concepts came up here and there during the design.\nThere is a lot that could be done to improve the project, skip lists could be optimized further, bloom filters could be made more cache efficient, and proper crash recovery could be implemented. I’ll perhaps update the code in the future.\nThank you for reading this far, feel free to get in touch for suggestions or clarifications!\nHave a nice day 😃\nReferences Designing Data-Intensive Applications Database Internals A Skip List Cookbook Complexity is not actually the same from a theoretical standpoint, indeed worst case time complexity is $O(n)$ for every operation on Skip Lists. This happens when we don’t create levels. ↩︎\nThere exist a lot of different encodings to store integers in a compressed fashion. Some of the most famous are $\\delta$ and $\\gamma$ codes by Peter Elias, Golomb coding and many more. Each one of them is better suited to a given probability distribution of integers. ↩︎\nIf you want to give this task a try, here’s an equivalent Leetcode problem. ↩︎\n",
  "wordCount" : "1998",
  "inLanguage": "en",
  "datePublished": "2023-11-12T00:00:00Z",
  "dateModified": "2023-11-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tomfran.github.io/posts/lsm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tomfran.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<head>
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1EM39PGLW2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-1EM39PGLW2', { 'anonymize_ip': false });
}
</script>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-1EM39PGLW2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tomfran.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tomfran.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Log-Structured Merge Tree
    </h1>
    <div class="post-description">
      An LSM Tree overview and Java implementation
    </div>
    <div class="post-meta"><span title='2023-11-12 00:00:00 +0000 UTC'>November 12, 2023</span>&nbsp;·&nbsp;10 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#memtable" aria-label="Memtable">Memtable</a><ul>
                        
                <li>
                    <a href="#searching" aria-label="Searching">Searching</a></li>
                <li>
                    <a href="#inserting" aria-label="Inserting">Inserting</a></li>
                <li>
                    <a href="#choosing-a-level" aria-label="Choosing a level">Choosing a level</a></li></ul>
                </li>
                <li>
                    <a href="#sstable" aria-label="SSTable">SSTable</a><ul>
                        
                <li>
                    <a href="#indexing-and-look-ups" aria-label="Indexing and Look-Ups">Indexing and Look-Ups</a></li>
                <li>
                    <a href="#bloom-filters" aria-label="Bloom Filters">Bloom Filters</a></li>
                <li>
                    <a href="#data-layout" aria-label="Data layout">Data layout</a></li></ul>
                </li>
                <li>
                    <a href="#putting-it-all-together" aria-label="Putting it all together">Putting it all together</a><ul>
                        
                <li>
                    <a href="#insertion-and-search" aria-label="Insertion and Search">Insertion and Search</a></li>
                <li>
                    <a href="#flushing-the-memtable-to-disk" aria-label="Flushing the Memtable to disk">Flushing the Memtable to disk</a></li>
                <li>
                    <a href="#tables-compaction" aria-label="Tables compaction">Tables compaction</a></li></ul>
                </li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a><ul>
                        
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>I studied LSM trees at university and after encountering them twice in
<a href="https://dataintensive.net/">Designing Data-Intensive Applications</a> and
<a href="https://www.databass.dev/">Database Internals</a> I decided to implement something in Java.</p>
<p>The idea behind this project is not to provide the most
efficient implementation ever, but to experiment with
storing data on disk, any suggestions are welcome!</p>
<p>Here&rsquo;s the <a href="https://github.com/tomfran/LSM-Tree/tree/main">Github repo</a> if you want to have a look,
this article is also published on <a href="https://medium.com/@tomfran/log-structured-merge-tree-a79241c959e3">Medium</a>.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>An LSM tree is a structure used by NoSQL databases, such as
Cassandra, RocksDB, LevelDB, Dynamo, and so on. It&rsquo;s suitable for write-intensive applications.</p>
<p>We can distinguish two key components of the tree, the
in-memory buffer, also called Memtable, and the disk-resident tables. The main
idea is to accept writes to the in-memory part of the tree, and to flush them
periodically, or when a certain size is met.</p>
<p>A key aspect of this structure is ordering, indeed, keys are sorted both in RAM and
on disk, enabling logarithmic searches.</p>
<p>For the sake of this project elements in the tree are simple key-value pairs.</p>
<h2 id="memtable">Memtable<a hidden class="anchor" aria-hidden="true" href="#memtable">#</a></h2>
<p>Having sorted elements in memory is not a new problem we can exploit any
efficient order-preserving data structure for this part, such as
<a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">Red-Black</a> or
<a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">AVL</a> trees.</p>
<p>In this particular implementation, I decided to build a <a href="https://en.wikipedia.org/wiki/Skip_list">Skip List</a>,
which provides the same theoretical complexity in the average case of balanced trees, but is straightforward to implement. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>A Skip List is a multi-leveled linked list. The idea is to have fast lanes between nodes, and, by
carefully constructing them, we can reduce the number of links we need to traverse while searching.</p>
<p>
<figure>
    <img src="skiplist.i.png"
        
        
        
            style="filter: var(--c-img-filter);"
        
    >
</figure>




</p>
<p>The list properties are:</p>
<ul>
<li>elements at level zero are sorted;</li>
<li>the number of levels are $\log(n)$, where $n$ is the size of the list;</li>
<li>if a node is at level $i$, then is must also be at level $i-1$.</li>
</ul>
<h3 id="searching">Searching<a hidden class="anchor" aria-hidden="true" href="#searching">#</a></h3>
<p>Given the above properties, searching is done as follows:</p>
<ul>
<li>start at the highest level and traverse until the node key is less than the wanted key;</li>
<li>if the successor surpasses the wanted key, go down a level and repeat, else we found the element.
Eventually, we&rsquo;ll reach level zero, and determine if the element is found or not.</li>
</ul>
<h3 id="inserting">Inserting<a hidden class="anchor" aria-hidden="true" href="#inserting">#</a></h3>
<p>Insertion proceeds as follows:</p>
<ul>
<li>locate the insert position with the same logic as before;</li>
<li>determine a level for the new element;</li>
<li>insert as in a linked list, but at each required level. Note that for this to work we need to
keep track of a predecessor buffer while descending levels. This way we can correctly replace successors pointers at each level.</li>
</ul>
<pre tabindex="0"><code>public void add(ByteArrayPair item) {

    // Locate the element keeping track of predecessors at each level
    Node current = sentinel;
    for (int i = levels - 1; i &gt;= 0; i--) {
        while (current.next[i] != null &amp;&amp; current.next[i].val.compareTo(item) &lt; 0)
            current = current.next[i];
        buffer[i] = current; 
    }

    // Replace current value if possible
    if (current.next[0] != null &amp;&amp; current.next[0].val.compareTo(item) == 0) {
        current.next[0].val = item;
        return;
    }

    // Insert new node at a random level, updating predecessors
    Node newNode = new Node(item, levels);
    for (int i = 0; i &lt; randomLevel(); i++) {
        newNode.next[i] = buffer[i].next[i];
        buffer[i].next[i] = newNode;
    }
}
</code></pre><h3 id="choosing-a-level">Choosing a level<a hidden class="anchor" aria-hidden="true" href="#choosing-a-level">#</a></h3>
<p>To determine a level, we can toss a coin and keep going until we get heads.
This would require a lot of random generations, a faster way is to generate a single
number and use its binary representation as boolean values.</p>
<pre tabindex="0"><code>private int randomLevel() {
    int level = 1;
    long n = rn.nextLong();
    while (level &lt; levels &amp;&amp; (n &amp; (1L &lt;&lt; level)) != 0)
        level++;
    return level;
}
</code></pre><h2 id="sstable">SSTable<a hidden class="anchor" aria-hidden="true" href="#sstable">#</a></h2>
<p>A Sorted String Table is a disk-based structure for sorted immutable data.
They consist of two main files, one with actual data and another with an index to speed up look-ups.</p>
<h3 id="indexing-and-look-ups">Indexing and Look-Ups<a hidden class="anchor" aria-hidden="true" href="#indexing-and-look-ups">#</a></h3>
<p>Given the data file, searching for a key can be implemented with a full scan. This is tremendously slow
on big files, hence we rely on indexing to skip portions of data.</p>
<p>Given a sampling factor $k$, we build a sparse index with keys at position $0, k, 2k$, and so on.
By storing the index in an array we can rely on binary search to find a given offset in the data file, where we can start
a linear scan. This permits us to skip a lot of unnecessary comparisons and locate a file portion that likely stores our value.</p>
<p>
<figure>
    <img src="sstable.i.png"
        
        
        
            style="filter: var(--c-img-filter);"
        
    >
</figure>




</p>
<p>Note that we can stop the search as soon as the current element surpasses the wanted one.
Below is the code for searching, this implementation is as lazy as possible, meaning that we only read
what&rsquo;s strictly necessary while iterating on the input stream.</p>
<pre tabindex="0"><code>public byte[] get(byte[] key) {

    // binary search an offset to start search
    long offset = getCandidateOffsetIndex(key);
    int remaining = size - sparseSizeCount.getInt(offsetIndex);
    
    // move input stream to the offset given by the index
    is.seek(offset);

    int cmp = 1;
    int searchKeyLen = key.length, readKeyLen, readValueLen;

    byte[] readKey;
    while (cmp &gt; 0 &amp;&amp; remaining &gt; 0) {

        remaining--;
        readKeyLen = is.readVByteInt();

        // gone too far
        if (readKeyLen &gt; searchKeyLen) {
            return null;
        }

        // gone too short
        if (readKeyLen &lt; searchKeyLen) {
            readValueLen = is.readVByteInt();
            is.skip(readKeyLen + readValueLen);
            continue;
        }

        // read full key, compare, if equal read value
        readValueLen = is.readVByteInt();
        readKey = is.readNBytes(readKeyLen);
        cmp = compare(key, readKey);

        if (cmp == 0) {
            return is.readNBytes(readValueLen);
        } else {
            is.skip(readValueLen);
        }
    }

    return null;
}
</code></pre><h3 id="bloom-filters">Bloom Filters<a hidden class="anchor" aria-hidden="true" href="#bloom-filters">#</a></h3>
<p>What happens when we search for a key that&rsquo;s not on disk? We waste a lot of precious CPU cycles
on binary searching and seeking on an offset, and iterating until we surpass the wanted key.</p>
<p>To avoid unnecessary operations we can rely on a compact and probabilistic structure such as <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filters</a>.
The idea is to have a structure that answers membership queries, having some false positive answers, but no false negatives.
We can tune the structure for our particular needs, by specifying a false-positive rate.</p>
<p>So, while looking for a key, we first test for probabilistic membership, and if the answer is negative, we can
early return null from the search.</p>
<pre tabindex="0"><code>public byte[] get(byte[] key) {
    if (!bloomFilter.mightContain(key))
        return null;
    ...
}
</code></pre><h3 id="data-layout">Data layout<a hidden class="anchor" aria-hidden="true" href="#data-layout">#</a></h3>
<p>Data is disk-resident, hence we need to define a binary format to follow, with minimal overhead.
An SSTable is made of $n$ elements, where each one of them has a variable length <em>key</em> and <em>value</em>.</p>
<p>Key and value pairs are byte arrays, and to lay them out on disk we encode their length $l$, followed
by $l$ bytes.
Each integer is written in <a href="https://nlp.stanford.edu/IR-book/html/htmledition/variable-byte-codes-1.html">variable byte encoding</a>,
to not waste 32 bits on small numbers. <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>This encoding uses a byte to store a continuation bit and a 7-bit payload containing part of the
represented number. For instance, consider the number $456$ and its binary representation $111001000$,
the variable byte encoded version is:</p>
<p>$$|1|0000011|0|1001000|$$</p>
<p>The first byte begins with one, hence that the number is not finished, while the second block starts
with zero, indicating no more bytes are needed to decode the current integer.</p>
<p>The index file is a list of keys, so we can use the same length plus payload encoding for it.
Each index entry has an offset related to it, specifying the number of bytes to skip in the
file to reach it. Those offsets are increasing, so we can use something like <a href="https://en.wikipedia.org/wiki/Delta_encoding">delta-encoding</a> to store them. Below is an example of such encoding:
$$0, 25, 76, \dots$$
$$\downarrow$$
$$0, (25 - 0), (76 - 25), \dots$$
Resulting integers are smaller, thus encoded in fewer bytes.</p>
<p>Finally, Bloom Filters are represented by some hyperparameters and a bit vector, that we can encode as is.</p>
<h2 id="putting-it-all-together">Putting it all together<a hidden class="anchor" aria-hidden="true" href="#putting-it-all-together">#</a></h2>
<p>We saw how to construct an SStable and how a Skip List works in memory, It is time to combine the
two to obtain the final engine.</p>
<p>The main components of the tree are:</p>
<ul>
<li><em>in-memory mutable buffer</em> or <em>Memtable</em>: a skip list as presented in the previous paragraphs, with a max size;</li>
<li><em>in-memory immutable buffers</em>: a list of skip lists containing memtables that need to be flushed to disk;</li>
<li><em>disk-resident tables</em>: a collection of SSTables obtained from memtables flushing, they are divided into levels,
level zero containing the most recent data.</li>
</ul>
<p>
<figure>
    <img src="tree.i.png"
        
        
        
            style="filter: var(--c-img-filter);"
        
    >
</figure>




</p>
<p>We are going to first see how primitives are defined, and then give an overview of how the tree is maintained,
with buffer flushing and table compaction.</p>
<h3 id="insertion-and-search">Insertion and Search<a hidden class="anchor" aria-hidden="true" href="#insertion-and-search">#</a></h3>
<p>To insert a new element we simply add it to the in-memory buffer. If the list does not exceed the maximum size we are done, otherwise the current list is scheduled for disk flushing, and the mutable buffer is re-initialized.</p>
<p>Searching is a bit trickier, and has at most three steps:</p>
<ul>
<li>query the mutable buffer;</li>
<li>query all the immutable buffers scheduled for flushing;</li>
<li>query all the disk tables starting from level zero and on.</li>
</ul>
<p>If, at any point, the wanted key is found, we can stop the search.</p>
<pre tabindex="0"><code>public byte[] get(byte[] key) {
    byte[] result;

    if ((result = mutableMemtable.get(key)) != null)
        return result;

    for (Memtable memtable : immutableMemtables)
        if ((result = memtable.get(key)) != null)
            return result;

    for (LinkedList&lt;SSTable&gt; level : tables)
        for (SSTable table : level)
            if ((result = table.get(key)) != null)
                return result;

    return null;
}
</code></pre><h3 id="flushing-the-memtable-to-disk">Flushing the Memtable to disk<a hidden class="anchor" aria-hidden="true" href="#flushing-the-memtable-to-disk">#</a></h3>
<p>When a given threshold is met, the Memtable is scheduled for flushing. To avoid blocking the whole
Tree until data is persisted on disk, we use a background thread.</p>
<pre tabindex="0"><code>private void checkMemtableSize() {
    if (mutableMemtable.size() &lt;= mutableMemtableMaxSize)
        return;

    synchronized (immutableMemtablesLock) {
        immutableMemtables.addFirst(mutableMemtable);
        mutableMemtable = new Memtable(mutableMemtableMaxSize);
        memtableFlusher.execute(this::flushLastMemtable);
    }
}
</code></pre><p>The background executor collects the older memtable to flush and creates a level-zero SSTable
on disk. It is important to guard critical sections while doing such operations.</p>
<h3 id="tables-compaction">Tables compaction<a hidden class="anchor" aria-hidden="true" href="#tables-compaction">#</a></h3>
<p>Flushing many Memtables on disk creates excessive read amplification, as we need to potentially query a lot
of different structures to find the wanted element.
One solution is to employ periodic compaction of disk tables.</p>
<p>The main idea is, starting from a collection of tables, to obtain a single merged one. There might exist
conflicts, that need to be resolved with a last-write-wins principle.</p>
<p>For instance, given the following
three tables, ordered by flushing time:</p>
<ul>
<li>$t_1 = [ a : 1, b : 2 ]$</li>
<li>$t_2 = [ b : 7, c : 3 ]$</li>
<li>$t_3 = [ a : 9, d : 9 ]$</li>
</ul>
<p>The result after merging will be:
$$t_4 = [a:1, b:2, c:3, d:9]$$</p>
<p>For the sake of this implementation, the compaction policy works by defining a maximum level size;
once a level $l$ reaches the threshold, all its tables are compacted into a $l+1$ level table.</p>
<p>Implementation-wise, this is equivalent to the problem of merging $k$ sorted iterators. The
problem can be solved by using a priority queue to find the next element in $log(k)$ time complexity. <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>Overall this was a really fun project, there were far more implementation
challenges than I expected and some cool DSA concepts came up here and
there during the design.</p>
<p>There is a lot that could be done to improve the project, skip lists could
be optimized further, bloom
filters could be made more cache efficient, and proper crash recovery could
be implemented. I&rsquo;ll perhaps update the code in the future.</p>
<p>Thank you for reading this far, feel free to get in touch for suggestions or clarifications!</p>
<p>Have a nice day 😃</p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ul>
<li><a href="https://dataintensive.net/">Designing Data-Intensive Applications</a></li>
<li><a href="https://www.databass.dev/">Database Internals</a></li>
<li><a href="https://api.drum.lib.umd.edu/server/api/core/bitstreams/17176ef8-8330-4a6c-8b75-4cd18c570bec/content">A Skip List Cookbook</a></li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Complexity is not actually the same from a theoretical standpoint, indeed worst case time complexity is $O(n)$ for every operation on Skip Lists.
This happens when we don&rsquo;t create levels.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>There exist a lot of different encodings to store integers in a compressed fashion. Some of the
most famous are <a href="https://en.wikipedia.org/wiki/Elias_delta_coding">$\delta$</a> and
<a href="https://en.wikipedia.org/wiki/Elias_gamma_coding">$\gamma$</a> codes by Peter Elias,
<a href="https://en.wikipedia.org/wiki/Golomb_coding">Golomb coding</a> and many more. Each one of them is better suited to
a given probability distribution of integers.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>If you want to give this task a try, here&rsquo;s an equivalent <a href="https://leetcode.com/problems/merge-k-sorted-lists/">Leetcode problem</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tomfran.github.io/tags/database/">database</a></li>
      <li><a href="https://tomfran.github.io/tags/java/">java</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://tomfran.github.io/"></a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
